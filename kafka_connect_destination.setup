connector.class=io.confluent.connect.s3.S3SinkConnector
value.converter.schemaAutoRegistrationEnabled=true
s3.region=<region>
flush.size=2
tasks.max=1
key.converter.region=us-east-1
value.converter.avroRecordType=GENERIC_RECORD
format.class=io.confluent.connect.s3.format.parquet.ParquetFormat
value.converter.compatibility=NONE
key.converter.compatibility=NONE
key.converter.avroRecordType=GENERIC_RECORD
value.converter=com.amazonaws.services.schemaregistry.kafkaconnect.AWSKafkaAvroConverter
s3.bucket.name=<bucket-name>
key.converter=com.amazonaws.services.schemaregistry.kafkaconnect.AWSKafkaAvroConverter
store.kafka.keys=false
schema.compatibility=NONE
topics=<topic-name>
value.converter.registry.name=<values-registry>
store.kafka.headers=false
value.converter.region=<region>
key.converter.registry.name=msk-connect-keys
key.converter.schemaAutoRegistrationEnabled=true
partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
name=<connector-name>
storage.class=io.confluent.connect.s3.storage.S3Storage
behavior.on.null.values=ignore
locale=US
timezone=EST
timestamp.extractor=RecordField
timestamp.field=<field-row>
path.format='year'=YYYY/'month'=MM/'day'=dd
partition.duration.ms=60